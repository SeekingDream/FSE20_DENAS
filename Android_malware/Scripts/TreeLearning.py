from Android_malware.Scripts.utils import *
from sklearn import tree
from sklearn.tree import _tree


FEANUM = FEANUMDIC[NowYear]


class Node():
    def __init__(self):
        self.Pos = None
        self.EdgeList = []
        self.NodeList = []
        self.Parent = None
        #self.Hidden = []

    def generateRule(self):
        if self.Pos == None:
            return []    #[self.Hidden]
        RuleSet = []
        for i in range(len(self.EdgeList)):
            SonRuleSet = self.NodeList[i].generateRule()
            if SonRuleSet != []:
                for sonrule in SonRuleSet:
                    rule = [[self.Pos, self.EdgeList[i]]]
                    rule.extend(sonrule)
                    RuleSet.append(rule)
            else:
                rule = [[self.Pos, self.EdgeList[i]]]
                RuleSet = [rule]
        return RuleSet


def decideLeafNode(data, label, feature):
    if (label == 1).all():
        return True, None
    index = np.nonzero(feature)[0]
    if len(index) < FEANUM or (data[:,index] == data[0,index]).all():
        rule = []
        for i in range(len(index)):
            rule.append([index[i], data[0, index[i]]])
        return True, rule
    return False,None


def calInforEntropy(data, label):
    pos_pk = np.sum(label) / len(label)
    neg_pk = 1 - pos_pk

    if pos_pk == 0:
        EntD = - neg_pk * np.log2(neg_pk)
        return  EntD
    if neg_pk == 0:
        EntD = -pos_pk * np.log2(pos_pk)
        return EntD
    return -(pos_pk * np.log2(pos_pk) + neg_pk * np.log2(neg_pk))


def selectBestFeature(data, label, feature):
    infEntropy = np.zeros([len(feature), 1])
    for i in range(len(feature)):
        if feature[i] == 0:    #this  feature has been selected
            infEntropy[i] = -100000
            continue
        else:
            possibleVal = getpossibleVal(data, label, i)
            for val in possibleVal:
                dataSubSet, labelSubSet = getSubData(data,label, i, val)
                infEntropy[i] -= calInforEntropy(dataSubSet, labelSubSet)

    bestPos = np.argmax(infEntropy)
    return bestPos


def getpossibleVal(data, label, pos):
    index = np.nonzero(label)[0]
    data = data[index,pos]
    dic = {}
    for d in data:
        if d in dic.keys():
            dic[d] += 1
        else:
            dic[d] = 1
    sorted(dic.items(),key=lambda item:item[1])
    valset = []
    for d in dic:
        valset.append(d)
    return valset


def getSubData(data, label, pos ,val):
    pos = int(pos)
    index = np.where(data[:, pos] == val)
    DataSet = data[index]
    labelset = label[index]
    return DataSet, labelset


def generateRuleTree(data, label, feature, height = 0):
    newNode = Node()
    isLeaf, Lag = decideLeafNode(data, label, feature)
    if  isLeaf == True:
        return newNode
    else:
        featurecopy = np.copy(feature)
        bestPos = int(selectBestFeature(data, label, featurecopy))
        if bestPos == None:
            return Node
        newNode.Pos = bestPos
        valList = getpossibleVal(data, label, bestPos)
        for val in valList:
            nextfeature = np.copy(feature)
            nextfeature[bestPos] = 0
            SubData, SubLabel = getSubData(data, label, bestPos, val)
            sonNode = generateRuleTree(SubData, SubLabel, nextfeature, height + 1)
            sonNode.Parent = newNode

            newNode.EdgeList.append(val)
            newNode.NodeList.append(sonNode)

        return newNode



def loaddata(filepath = "data\\train_model_data.pkl"):
    f = open(filepath,"rb")
    data = pickle.load(f)
    X = data[0]
    y = data[1]
    return X, y



def tree_to_code(tree, feature_names):
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]
    #print("def tree({}):".format(", ".join(feature_names)))

    def recurse(node, depth, rule):
        indent = "  " * depth
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            #print("{}if {} <= {}:".format(indent, name, threshold))
            rule_1 = rule.copy()
            rule_1.append([name, 0])
            R1 = recurse(tree_.children_left[node], depth + 1, rule_1)

            rule_2 = rule.copy()
            rule_2.append([name, 1])
            #print("{}else:  # if {} > {}".format(indent, name, threshold))
            R2 = recurse(tree_.children_right[node], depth + 1, rule_2)

            R1.extend(R2)
            return R1
        else:
            #print("{}return {}".format(indent, tree_.value[node]))
            if tree_.value[node][0, 1] > tree_.value[node][0, 0]:
                return [rule]
            else:
                return  []

    RuleSet = recurse(0, 1, [])
    return RuleSet


def generateDTreeRuleSet(x, test_num, maxdepth):
    index = np.random.choice( np.arange(len(x)), test_num)
    x = x[index]
    if NowYear == 2019:
        try:
            pred_y = np.load("../data/2019/prediction.npy")
        except:
            model = loadModel(year=NowYear)
            pred_y = (model.predict(x, batch_size=1000) > 0.5)
    else:
        model = loadModel(year = NowYear)
        pred_y = (model.predict(x, batch_size = 1000) > 0.5)
    clf = tree.DecisionTreeClassifier(max_depth= maxdepth, random_state = 0) #
    clf = clf.fit(x, pred_y)
    RuleSet = tree_to_code(clf, np.arange(FEANUM))
    return RuleSet






if __name__ == '__main__':
    x, y = ReadData(year=NowYear, IsTrain=True)
    RuleSet = generateDTreeRuleSet(x, test_num= 10000, maxdepth=50)
    for rule in RuleSet:
        print(rule)
    f = open("../RuleSet/" + str(NowYear) + "tre.pkl", "wb")
    pickle.dump(RuleSet, f)
    f.close()
