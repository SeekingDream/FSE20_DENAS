import keras
from keras import Model
import numpy as np
import keras.backend as K
from keras.models import load_model
import pickle as pickle
from keras.layers import Dense, Activation, Dropout, Layer


TerminationLength = 10
THRESHOLD = 0
VECSIZE = [0]
FEANUM = 134
BIT = 2



class RuleStructure():
    def __init__(self, dataset, rule):
        self.dataset = dataset
        self.rule = rule
        self.predy = 0
        self.size = len(self.dataset)

    def decideRule(self):
        if TerminationLength == len(self.rule) or (self.predy > 0 and len(self.rule) >= 5):
            return True
        else:
            return False


    def SetPredy(self, predy):
        self.predy = predy


    def SplitNode(self, newpt):
        pos = int(newpt / BIT)
        val = newpt % BIT
        selindex = np.where(self.dataset[:, pos] == val)[0]
        if len(selindex) == 0:
            return None
        else:
            newrule = self.rule.copy()
            newrule.append(newpt)
            newdataset = self.dataset[selindex].copy()
            NewNode = RuleStructure(newdataset, newrule)
            self.dataset = np.delete(self.dataset, selindex, axis=0)
            self.size = len(self.dataset)
            return NewNode

    @property
    def __eq__(self, other):
        return len(self.dataset) == len(other.dataset)
    def __lt__(self, other):
        return -len(self.dataset) < -len(other.dataset)


class ActivePossible(Layer):
    def __init__(self, ac=None, **kwargs):
        # self.theta = theta
        # self.alpha1 = alpha1
        # self.alpha2 = alpha2
        self.ac = ac
       #self.activate = activate
        super(ActivePossible, self).__init__(**kwargs)

    def call(self, x):
        return K.cast(x, K.floatx()) * self.ac

        # fx_0 = K.relu(x)  # for x>0
        # fx_1 = self.alpha1 * x * K.cast(x > self.theta, K.floatx()) * K.cast(x <= 0.0, K.floatx())  # for theta<x<=0
        # fx_2 = self.alpha2 * x * K.cast(x <= self.theta, K.floatx())  # for x<=theta
        # return fx_0 + fx_1 + fx_2
        #return keras.layers.Multiply([x, self.activate])

    def compute_output_shape(self, input_shape):
        return input_shape

    def set_ac(self, ac):
        self.ac = ac


def set_acpos(model, ac, index):
    for i in range(len(ac)):
        model.layers[int(index[i])].set_ac(ac[i])
    return model


def getPuppetModel(modelname):
    m = load_model(modelname)
    model = keras.Sequential()

    model.add(Dense(200, input_shape=[FEANUM], activation= None))
    model.layers[-1].set_weights(m.layers[0].get_weights())
    model.add(ActivePossible(ac = np.ones([200]))) #############1111111###############


    model.add(Dense(200, activation= None))
    model.layers[-1].set_weights(m.layers[3].get_weights())
    model.add(ActivePossible(ac=np.ones([200])))  #############1111111###############


    model.add(Dense(1, activation= None))
    model.layers[-1].set_weights(m.layers[6].get_weights())

    return model


def saveRuleTxt(fname, rule):
    f = open("..\\RuleSet\\" + fname, "a")
    for val in list(rule):
        f.write(str(val))
        f.write(" ")
    f.write("\n")
    f.close()



def Normalization(X):
    X[np.nonzero(X)] = 1
    return X


def loaddata(filepath = "data/train_model_data.pkl"):
    f = open(filepath,"rb")
    data = pickle.load(f)
    X = data[0]
    y = data[1]
    y = y[:, 0]
    X = Normalization(X)
    return X, y



def calContributionVec(puppetModel, activationPossible):
    activationPossible = activationPossible.reshape([2, 200])
    puppetModel = set_acpos(puppetModel, activationPossible, [1, 3, ])
    contribution = getGradient(puppetModel)
    return contribution[0]


def getActiveNode(lay_0, lay_3,seed):
    dataNum = len(seed)

    activationNode = np.zeros([dataNum, 400])

    activationNode[:, 0 : 200] = \
        lay_0.predict(seed, batch_size= 20000).reshape(dataNum, 200)

    activationNode[:, 200 : 400] = \
        lay_3.predict(seed, batch_size= 20000).reshape(dataNum, 200)

    return activationNode



def getActivateState(model, x):
    lay_0 = Model(inputs=model.input,
                  outputs=model.layers[0].output)
    lay_3 = Model(inputs=model.input,
                  outputs=model.layers[3].output)

    activationNode = getActiveNode(lay_0, lay_3, x)
    return activationNode



def calAcStateFromRule(nowrule, model, testNum = 1000):
    data = np.random.randint(0, BIT, [testNum, FEANUM])
    for r in nowrule:
        pos = int(r / BIT)
        val = r % BIT
        data[:, pos] = val
    acstate = getActivateState(model, data) > 0
    acstate = np.mean(acstate, axis=0)
    return acstate



def calPredy(contributionVec, rule):
    y = 0
    rulepos = []
    for r in rule:
        rulepos.append(int(r / 2))

    for i in range(FEANUM):
        if i not in rulepos:
            y += (contributionVec[i] / 2)
    for r in rule:
        pos = int(r / 2)
        val = r % BIT
        if val == 1:
            y += contributionVec[pos]
        else:
            y -= contributionVec[pos]
    return y

def getGradient(puppetModel):
    output = puppetModel.output
    input = puppetModel.input
    gradients = K.gradients(output, input)[0]

    out = K.function([input], [gradients])
    x = np.zeros([1, FEANUM])
    y = out([x])[0]
    return  y



def ReadRuleSet(fileName):
    f = open(fileName, "rb")
    RuleSet = pickle.load(f)
    f.close()
    return RuleSet


def ReadtxtRule(fileName):
    RuleSet = []
    f = open(fileName, "r")
    R = f.readlines()
    f.close()
    for r in R:
        rule = r.split(' ')[:-1]
        RuleSet.append([(int(int(i)/BIT), int(i)%BIT) for i in rule])
    return RuleSet